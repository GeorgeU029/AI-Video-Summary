 In this lecture, we're going to discuss context free barmers and parsing. Context free barmers are essentially rules to define the syntax or programming languages. Thus, they are program generators. Parsers, on the other hand, push the automata. They recognize the language defined by context free barmers. Let's look at the context free grammar. Context free barmer can be defined by a set of four topos. We can look at the basic way. There's a set of terminals. We use t to stand for that. A set of non-terminoes. We use n to stand for that. A style symbol, a single symbol. Basically, s, which is non-termino. A set of production rules, where each rule is the form of x and the derive to w. The x is a non-termino. W can be a sequence of non-terminoes and terminals. Let's look at a simple example, and which is expression grammar. This grammar is defined by a typical arithmetic expression. We have five rules, actually, more than five rules, because some rules are combined. You can see we have expression, term, factor, add, operation, and multiplication operation. These non-terminoes appear on the left hand side of these rules. They have to be further replaced by the right hand side. Then we have term, occur on the right hand side of this rule, which is non-termino. Then we have this straight bar. We understand this is all, which means there's an additional rule here on the right hand side, which has expression. Addition operation and a term. It can see the second rule, basically, the non-termino expression appeared again. It appeared at the very left hand side. This grammar, if we define something recursively in terms of sub, and we call it out left-air recursion. If you remember, we talked about the regular expression where we can define repetition. We use clean start to define repetition. Where is additional power of context free grammar? Essentially, it's through the recursion. We have this recursion. You can define expression in terms of sub. This grammar is left-air recursive. Then we have these different rules. These different rules actually capture the precedence. That means which operation occurs before some other operations. In this example, you can see multiplication operations appeared at the different level from the addition operations. Actually, the way we arrange that, actually, the higher the level of rule actually has weaker precedence. Basically, that means at the level 5 here, these operations have high precedence. They will be derived first, or they will be used to combine the other things together first, then the rules above them. Not only we have precedence, also we have sensitivity. Basically, if we have left recursion, the grammar actually defines the left associativity. You combine things from left to right. Based on grammar, we can apply the rules to obtain derivation. Deirization essentially means how we generate the sequence. How we put the sequence into some kind of order according to grammar rules. One step derivation essentially is application of one rule by replacing the left-hand side, the non-terminal, with the right-hand side expression, which can be a combination of terminals and non-terminal. Deirization sequence consists of a sequence of these single-step derivations. We typically use this double-line error, plus means at least one rule, use the term zero or zero more steps. From these derivations, we can obtain the language defined by a grammar. The language defined by a grammar is a sequence of symbols, which can be derived from the start symbol, using one or more rules. Of course, this sequence needs to be all terminals. Non-terminal are used to further define some sense. They are not possible language, but terminals actually consist of the elements of a language. We understand the language, of course, the grammar defined the language. Language can have many, many sentences of these sequences. For each sequence, we can use a pass-tree to represent that. For each sequence, we can have a derivation. Of course, this sequence needs to be a valid sentence in the language. If it's a valid, then we can derive a pass-tree to represent this derivation. Here are some examples for the arithmetic expression grammar. In this particular case, we have three plus four multiply five. You can see we have this pass-tree based on grammar rules. For every application, a grammar rule basically adds one level to the tree. For example, for example, from expression, there is a rule which consists of expression, add, operation, and a term. We apply a one rule, we generate one level, and then from expression to term, you see one derivation rule, we generate another level, and a so on and so forth until we reach the terminal, which is numerical number three. Here, we have addition operation and a so on and so forth, and the precedence is refracted by the level of this tree. You can see here the multiplication takes precedence over addition, because here you have to actually calculate this branch first. To reach the term, then this term will be used in the computation of this addition operation. Here is another example which shows the left associativity, and basically you can see since the grammar is left recursive. The passing will generate this kind of tree, which basically says we compute 10 minus 4 first, and then followed by minus 3, because this branch is lower. That means it needs to be completed or reduced first before we can get the value, and which can be used to compute the overall expression. We have seen one good grammar, which is basically left recursive, refracts the precedence and associativity, and of course not other grammar possess that kind of attributes. We will see some grammar and big use. It is important to understand the grammar when we say grammar is ambiguous. Basically you can find some string or some sentence, which can produce two past trees, which have different structure. In this particular example, let's say we have this grammar, basically the expression grammar, we have some identified I, then we have addition, subtraction, multiplication, division, and so on and so forth. All these rules are at the same level. If they are at the same level, that means you cannot reflect that kind of precedence of the operation. So you end up for this particular expression, you can generate these two past trees. The other way is that basically the operator precedence in refracting this grammar. We talk about different levels. Also, associativity, because this grammar tends to be both left recursive and a right recursive. You see, it appears that at the leftmost side, as well as rightmost side. You can either have left recursion or you can have right recursion, so that constant problem. But if the grammar is ambiguous, in most cases we can turn that grammar into non-ambiguous grammar. But you need to be aware when we say the grammar is ambiguous, basically we can't have two past trees. But the grammar is ambiguous, doesn't mean the language is ambiguous. No matter how they try, you cannot get ambiguous grammar. These are two different concepts to make sure you understand that. Typically, if the grammar is ambiguous, we can introduce precedence as well as associativity to resolve the problem to make the grammar ambiguous. Essentially, we use a different non-terminal for each precedence level. We always start a rule with the operator with lowest precedence and then a rule for the operator with the next lowest precedence and so on and so forth. That is basic idea. If we want the left associativity, we can use left recursion. If we want the right associativity, then we can use right recursion. Now, the previous grammar can be changed now to this grammar, which is left recursive. The left associativity is the precedence. We have the highest precedence is reflected by the last rule and then move upwards. And you can't think about when you have the rules at the top, you can see the derivation will appear at the high level of the tree. And however, in terms of computation, what happens is basically you go from the leaves. Then you go gradually towards the root. So that means the root basically has low precedence. That means what happened at the time of dealing with elements at the root will be much later then what happens when we deal with the leaves. So now we have some idea about what is context grammar. So how can we define a language? Now once we have context free grammar, we need to do the parsing. So basically try to do the other way around to try to recognize the language defined by context free grammar. So basically we try to define a generalize the parsers for the language. And of course there can be many different grammars for every context free language. So you can generate your own grammars basically or eventually define the same language, but not all grammars are created equal. That means some are better in terms of processing they are more efficient or maybe more do stumble. Some grammars are probably not that good for example and big use. So how we can build the parsers essentially in different ways. We know we can create a parser which runs in big old notation. I guess some of you may know this. Some of you may not know this. Big old essentially means upper bound. And so the time bound basically is here and cube and stand for the size of input. So it's a polynomial. And it runs okay but it's not fast enough. We'll see some special average special class for grammars which can be more efficient in most cases that will be much more desirable. So there are two well known parsing algorithms which can achieve this kind of performance. And based on their inventors when it's called the earliest algorithm. Other is cookie younger cassami algorithms. And so these are well known algorithms but they are not very efficient. So how can we get more efficient grammars. Okay essentially we look at some some classes. So we look at what can be defined using a compact grammar and which can produce very efficient parsers. So there are two major classes. And one is called LL. And first L stands for left to right. Okay so you scan for left to right. And L basically means the left the most derivation. So you need to look at left most non terminal. There you replace that with the right side expression. So the derivation always tackles left most non terminal. These parsers also call the top down are predictive parsers. So you derive from top because you try to predict something. So you build a pass tree from the root to the leaves essentially. And one thing you need to understand is when we talk about LL also else in a most stand for left to right. Which are actually applicable to right to the recursive ground. It's not left to recursive ground. So I always found it's still in to get confused about this in my previous offering of this class throughout many many years. Okay so make sure you stand back. So why it's left the recursive grammar is not suitable for left to most derivation. And one thing is obvious is basically if you have left the recursion. If you try to replace it, you will replace again again again again infinitely monetize. Because there's no end right to towards that. So that's why when the grammar is left to recursive. So you basically cannot use left to most derivation. So you have to use right most derivation. So that's exactly what this passing technique is for. So LR techniques are for. So it's for right most derivation parsers. Okay. Which are bottom up shift the reduce. So that means they build up a pass free from bottom up. And this type of passing techniques are applicable to the left to recursive ground. Because you are handling the left recursion at the last moment when you got every information you need. Okay. From the right side. So that's why once you have that, you can do reduction. Okay. So that is the major idea. So you need to understand. So. The left. The recursive grammar is only a property for LR parsing. If it's the right recursive grammar, we can use LL parsing techniques. So for LR parsing, there are also special classes like SLR, which is simple R and then look ahead. LR techniques. And often when you see LL or there's a number in the parentheses of update. Okay. And so this number essentially is used to indicate how many tokens you need to look ahead. Okay. So if it's one. So besides the token you are processing at the moment, you need to look ahead when more token. And after this one. Okay. So to make a decision. Okay. So that is important. Number of cells. Give you some idea about how many characters we need to look ahead. So in most cases, this look ahead by one token is adequate. But in some case may not be. Okay. So it's a very similar to the regular expression where when we try to build the tokens from the character sequence, we also need to look ahead to the characters. Here we look ahead for the tokens because now we have more meaning for building blocks for the language. So this basically tells you something about the relationship between different grammars. Okay. So LL1 grammar is also R1 grammar. So the right recursion will cause problems for LL1 grammar. So it just discusses the base. It will create a very deep stack. So you can't predict it repeatedly using the same rule. Okay. Essentially. Okay. And for every contest programmer, you can pass deterministically. And which has SL1 grammar. Okay. Of course, which is subset, a simple version of LR. So if you can pass a contest three grammar, which has something called the prefix property. It certainly means no value to see string is a prefix of another value string. If the grammar has this property, then you don't even need to look ahead because everything you see is unique. So there's no ambiguity. So there's no reason. No need for you to look ahead to determine and resolve the problem because itself provides the adequate information for you to make a decision. So let's look at some more meaningful examples here. Okay. So this is a grammar context free grammar for the LL1 passer, passing technique. So in this case, we have a program, which is a statement list. A statement list is statement followed by a statement list. And each statement can be as a lesson there, a assignment, a read statement, write statement. And then for the assignment statement, you have expression for the defined what are the expressions and so on and so forth. There are few more rules in the next page. If you pay attention to this grammar, you can easily see this grammar as right recursive because you see the statement list appears again on the right hand side, but this is the right most right. So it's right recursive as we already discussed the right recursive grammar in suitable for LL passing. And again, you will see here, turn tail, turn tail basically appears on the right most. So it's right recursive grammar and it's suitable for LL passing. So these are the remaining rules of this grammar. So like the bottom up grammar, this one captures the associativity and precedence. And however, it doesn't look very neat. So for one reason, let's quickly go back. See basically here, if we look at the factorian factor tail in this case, which studies multiplication operation with a factor and a factor tail, and then it has another rule which is empty. So you can't think about the basically for us, when you have multiplication, multiplication operation, the operator is in the middle, right? You have something on the left, something on the right, then you have operation in the middle. But in this grammar, the operation is actually defined in front. And the other part basically now is in the previous rule. So you see you basically break the expression is in two rules. So you have factor, factor tail, which involves operator and another operand here. So it's not natural for human to understand. However, this kind of grammar, right recursive grammar is very nice for LL passing. So basically you can produce very simple passers. So to handle this kind of thing. So we do that, build a pass tree by doing it from incremental approach. So let's look at the concrete example. Now we have that grammar. Now we can have a program defined using that grammar. And then we can see how we can build a pass tree from this program. So make sure you understand this, also concept. So when we have a grammar, we don't have a pass tree. So grammar is generic, right? A grammar defines essentially infinite many strings or sentences. So when we talk of pass tree, it is only associated with a particular string, a particular program. So you have a programming language, you can write any program you can imagine. So each program can be passed to generate pass tree and then to generate code eventually. So you don't generate code for language, because that doesn't make sense. So you generate code for a program written in that language. So now let's look at this particular program so we can build a pass tree for this. And here's the pass tree essentially using those 19 rules we just saw basically using the LL pass and technique. So you can see basically we start with it's a top down right. We start with a program and we put that into the stack essentially, you know, we have pushed on a Thomas, right? With that into stack, then we match the read because read is the first statement, right? So match the read. Now we push the statement list in and then we do that again again, which tells you exactly how we do that. We basically have a table or see a table in the next slide. So what happens in passing essentially have a big loop. So you repeatedly look up for an action to dimensional table based on the current left most non terminal case in front always because left most derivation we always focus on the left most non terminal with the current input token we might need to look when more token had. So what actions what the passers do, so the passers not only do that, but also search for something at the same time make decisions and either match terminal. So if you match terminal, you pop the stack. If you, so that's one action, okay? And of course, once you pop a stack, you can directly build that into the tree or you can save it somewhere then you build a tree at the very end. All you predict a production, okay? So again, when you predict the production here, you basically pop the non terminal in the stack and there you push the right side of the rule into the stack. Okay, so if none of those, for example, you cannot match a terminal or you cannot find production rule to predict. In this case, basically the syntax is wrong. So you have to produce error message case. So in this case. So here's a table actually which tells you what to do. Okay, so let's say this is the top of stack, you have a non terminal here and this table only refracts the prediction product because the matching product is trivial. So you have program now if this is on top of a stack and then if you see the ID here and you apply production rule, what? Okay, so basically which tells you now you have a statement list, you have to pop, you have to push down the statement list. And so if you have statement list, you see ID, not basically use rule two, which basically says it's a statement list and a statement. Okay, so also force. So if you want to check the details, you can go back a few slides and match that with the rules. Okay, so these are the rule numbers here and which tells you what to push to the stack. Essentially replace the top element of the stack with the right hand side expression and push those onto the stack. Okay, you keep doing that. That's for prediction. Okay, these are all telling prediction, but if you match the terminal, which are not shown here, then you just pop up the terminal and so whatever you pop up, these need to be saved was it's terminal, non terminal and eventually these will be used to build a pass tree. Of course, you can build a pass tree at the same time. It basically it's a strategy, whatever you can do and then make any difference. So as I just mentioned, to keep track the level most, non terminal, you push something you have not seen, the right side expression of a rule onto the stack. And you can see a little more details in the textbook in this figure. And so it is important to remember basically the stack will contain all the stuff you expect to see between now and end of program. Okay, of course, not at once, but dynamically. So you add something and a pop up something, you push down something, pop up something, whatever you pop up, basically I used to build this derivation tree or pass tree. Okay, so what happens if we want to do one pass in, but the given grammar is not given grammar is not recursive, it's left recursive. So what we have to do is first we have to convert this grammar into equivalent right recursive grammar. And then we can't plot LL1 passing case in this case. So for example, in this case, if we have ID list, goes to ID list, ID list, followed by ID, okay, so you see you can have a sequence of IDs here, but this rule itself is left recursive, okay, because ID list in the second rule appears on the leftmost side, okay. So it's a left recursive, if it's a left recursive, it is not suitable for LL passing, okay, so in this case. So however we can basically mechanically translate that into ID list, ID list, the tail, and then ID list tail, that was the comma, and then followed by ID, and followed by ID list tail, all empty, okay. So now one rule in the left recursion is translate into, you know, essentially three rules, right, one, two, and three, the last one is a blank, okay. And which now is right recursive, okay, so you have ID list, tail, ID list, tail, which appears on the rightmost side of the rule, okay. And so by doing that, now you can't apply the LL passing, okay, to solve the problem, okay, to successfully generate the pass tree. So one thing is how we can turn the left recursive grammar into right recursive graph, okay. And another technique is to deal with common prefix, okay. So you know, basically if we have to look more than one token in this case, then we have trouble, right, so we cannot determine, okay, so we can only look at. At the most one, so the reason we need to look ahead is because you have the same token, but the token behind it can determine different decisions in this case. So it's not unique. So in order to make sure our grammar can process that by looking just one token ahead, so we can do something called a left factory, okay. Here's example, okay, so have statement, okay, a statement is defined by either assignment, statement or a function called, okay. So you see ID here, followed by assignment operation, and ID here, followed by a left surprises case. So if you just look at ID, you cannot tell what to do, okay, whether it's assignment or whether it's function called, you have to look at one token ahead, right. So we can reserve this problem by left factoring, so we have statement ID, then we have ID statement tail. So in ID statement tail, now we can have whatever remaining part of this rule we put here and a remaining part of this rule, we put behind this, okay. So now, so everything is unique, okay. So we basically process that, we can do a derivation and push this ID statement tail onto the and then whenever the prime ID statement tail meets with this assignment operation, then and now basically it's assignment statement where it sees the left press and it knows it's function called, okay. So this can be done again mechanically, okay. So we basically can transform a grammar by left factoring to make sure, you know, the non-uniqueness thing is resolved, okay. So think in our homework assignment, you will have a question related to this, so make sure if you have questions, you can go back and visit this and just get some concept and then if you still have doubt, you can read a little more to understand that. Okay. However, okay, so these are the common problems if we, let's say we animate left recursion, common prefix and it may not always work, okay. So that depends on whether the language itself is is non-LL or not, okay. So essentially there are infinite many language which are not LL, okay. So no matter what you do in a resolve left recursion, common prefix, you still can pass them using LL techniques, okay. So but in most cases, you know, we don't have that common problem if we have, in some language to have that problem, we can't always, you know, use some special tricks to resolve them. Okay, so here are some examples in how a language may not be LL1 and so one famous example is called a dangling else, okay. So in this case, okay, it doesn't matter how many tokens you look ahead, you cannot still determine how you do it, okay. And of course, this actually appeared in some real programming language, you know, Pascal, it's a well-known language. And the reason it's ambiguous, that means you cannot make it this way, okay. So let's look at what does this mean exactly, okay. So you have a statement which is if statement, okay. So we understand the keyword if, or by some kind of condition, then we have a Zan class or else class, okay. And then you can't have other statement, you know. But let's focus on this if statement, okay. So the Zan class can have Zan and followed by statement. The else class now has else and statement, okay. So that's not problem. But it can also have option called epsilon, okay. Absalom basically here means empty, right. So if the else part is empty, that costs problem because we can't have if Zan some statement, okay. So let's say another if, okay. So if condition, then if condition, then statement, else statement, okay. So we don't know that else statement should appear there with the the first then or the second thing, right. So that costs confusion because the grammar allows that, okay. So there's, you know, maybe intentionally when somebody write a program, they may not aware exactly that will cause problem for the compiler to determine, okay. So all we can do is basically we can always deal with this kind of problem, by, you know, say using some special rules, okay. Also the grammar is ambiguous, okay. But we can always, you know, treat the use some special rules to deal with that. For example, we, you know, always, you know, pair the else to the closest then, okay. So in this case, of course, the second is then. So there is an ambiguity. Or more generally, we can have two rules, okay. So basically there are two rules, right. Then have one, the else, you know, which has the else part of the anti one, okay. So we can't always use say the first one to use. So in this case, again, we look at else, we always pair that with the second thing, okay. Because we use the first rule in the else part, else class, okay, which is the else statement, not the absinal, not the empty one, okay. So this is how we deal with some intentional design for us in the language. But, you know, in some modern languages, okay. So this problem can be resolved by just to introduce these explicit delimitators for the statements. For example, you try to always have an end to close if statement. And of course, in some languages, not necessarily using end, is, you know, if or in an FYK. So the reverse spelling of if, okay, to signify that's the end of this particular statement. If you use that, so you will never have an ambiguity occur in your project. So we understand now what does our pass it mean. And basically, it has push not tomat, then you have these rules, you do prediction, then use to pass it, okay. So actually, how we can actually build these, you know, predict the set, okay. So basically, say, you know, build that table, right. We have that table and predict what to use. And so we can use something one known basically calculated some use these functions. Got a first, for all. And to predict compute these to for the symbols, okay, which include nonterminal term symbols and then follow just for nonterminal symbols, okay. So these functional well, and which are defined by these rules essentially, and a first of this rule alpha, okay, you focus on this left hand side. Basically, says if you have a sequence of derivation, it's the, you know, first, whatever, nonterminal terminal, okay, A and a collected, it's the first appear there. You can appear in the first deal with derivation. And then if you have this case, so follow up case, so in this rule, it's from the the star symbol, okay, so you can have a sequence of alpha, okay, so some, some strings there, some tokens there. And so what follows this A is again, this A, okay, the small A of the capital A, okay. So you can define this and then of course, you can use the predict the set calculate and the combination of first and follow, okay. So these are one, you don't need to actually look at details, but if you want to, okay, details, these are computed actually there, quite straightforward. And here are some examples from that language we just saw. And for example, I just mentioned 15s, okay, so, so let's say you have a statement, statement list, okay, so in this case, what follows statement list, okay, so the first in the statement list, basically, you only have, have basically the empty, right, empty symbol, okay, so no other IDs are the two-minute ones, okay. But if you look at the first in the statement, okay, so first in the statement here, basically in the assignment statement, you have ID, okay, so that appears the first in this statement, okay. And similarly, in this statement, the first is the read, and in this first is right, okay, so we have some idea of a first, and let's look at the way it follows, okay, so what follows ID here, what follows ID is this assignment symbol, right. And in this statement, what follows ID, what follows the read is ID, okay. So these are actually fairly straightforward, and easy to understand, okay, of course, the detailed computation is a little involved, but first these programs, they are for machine to process enough for human to read. And this is the continuation, so these actually rules are all built by these functions, okay, so you predict it will generate this set of rules, and find out what's in verse, you know, set, you know, first, okay, follow, and predict the end of song, so forth, okay. So when we do our passing, if an token belongs to the predictor set of more than one production, with the same left hand side, then the grammar is not LL1, okay, of course, in this case, it's very easy to understand, okay, so if you have some token belong to the predictor set, one more production in this case, and then you don't know which production to use, right, because it costs ambiguous. A conflict can also arise because same token belong, begin more than one right hand side, or as a case, you know, it can also appear off the left hand side in some valid program, and for example, like absolute, okay, the empty one, okay. So these are just some general rules of the apply those functions to compute these, and you gather the information, then you can do a check, and I see what's the grammar actually is suitable for LL1 passing or not. Okay, so we have some idea about LL passing, okay, so the other one, we're going to briefly mention is our passing, okay, so this is pretty much table driven, okay, so very similar to the LL posture, but the LR posture actually is a little more complex, okay, so still it's a loop, you know, keep looking for the next reduction, okay, application reduction rule, and in this case, it's inspect the two dimensional table to find what action to take, just like the LL passively saw tables there, okay. However, unlike the LR posture, the LR driver has a non-trivial state machine, okay, so basically in the LR posture, it's very simple, in this case, because you just do prediction, prediction, okay, you don't, you know, need to keep memorizing things, okay. And in the LR crossing actually, so you need to have the termist finite state atomic to keep track of where you are at this moment, okay, so because it's in some sense, it's a bottom up, so you don't have the overall picture, okay, so you need to know the context, okay, which plays currently is being reduced, okay. So this is indexed by the current input token and current state, okay, so it's a little more complex than the finite state machine we discussed during the scanning, so we only need to have the input character, but here we need to have token and a current state, okay, so we need a bolt, okay. And so state contains a record of what has been seen so far, okay, not what is expected, okay, so it's very different, okay, so here, what's in the stack is what you have already seen so far, okay. And it's not like the stack in the LR crossing, in the LR crossing where we put something we haven't seen yet, okay, so this is a little bit different, okay, so make sure and understand that, what's in the stack is different because when it's top down, when it's bottom up, okay, so we say different type of information. So basically push dot, how much can be defined, you see, a state diagram, okay, so we have basically a state machine and a stack, okay, so we, you know, push dot, how much is basically, it's a deterministic state machine plus a stack, okay, so that's why you see the power of push dot, a atomic, which is, you know, deterministic finite state machine plus stack, so you need to have this additional mechanism, storage mechanism, okay. So there is no difference, essentially, when we look at the state diagram of deterministic finite state okay, except the arcs are now labeled, the way input symbol as well as the top of the stack symbol, okay, so you need to not only look at input token, which you're considering right now, but you also need to look at the top of the stack, okay, so what you try to, you know, do at this moment, okay, so, so there are a pair of input, okay, so in this case, and then you make a decision, either you push in pop, and you know, for the symbols onto of the stack, actually, you can do more than one, you can push in sometimes because you need to push a bunch of things into, or you can't pop a bunch of things when you do the reduction, you will pop multiple symbols to to satisfy a rule, when you push, you could also push more than one symbol into the stack. So we just mentioned that the LL also is pushed down atomic, but in this case, the state machine sub is very simple, okay, so we really don't need to remember much because everything can can be inferred from the stack, so the stack actually contains more information we need because we do the leftmost case, so we always, you know, do whatever we can do and do a reduction, okay, so, so you don't, the context in this case is much smaller, okay, so you always do the current state, you can make a decision, then you do a reduction, okay, so you can process that, okay, however, when you do LR, so you have to basically push a lot of things into the stack, then, you know, because the rightmost, right, rightmost means, you know, if you don't see it yet, everything need to be saved on the stack, then when you reach the rightmost, uh, non-terminal, then you can do the reduction, but by that time, you, lots of things are already on stack, so now you need to figure out, you know, how much you need to, to process, you know, at a time, okay, but when you have LL, basically the leftmost case, so you just look at the current top of and whatever the input that you make, make a reduction, in that case, you make a prediction, okay, so typically, you can imagine, you know, the push dot, atomic, in the LL, LL passing is, is, shallow, that means you don't need to say much onto the stack, but in LR passing, you have to essentially save almost everything into the stack, the stack will be very big in the very beginning, because you need to see the rightmost one, okay, and then you start to reduction, okay, so you need to, you know, normally it has a deeper stack, okay, when you do our processing, okay. So, a build is basically a PDA, you have multiple states, okay, so builds the pass tree from bottom up, okay, and need to keep track, which production might be in the middle, that's, that's, that's what I just mentioned in the middle, okay, so, so you need to aware a larger context in this case, okay, so this, this same machine we call characteristic, okay, finite state machine is used, based on shift and a reduce, basically, you know, you use that, the state decide, you know, was it, we should, the shift or was, we should reduce, okay, and then we have the same language, but defined by a slightly different grammar, of course, you know, when we talk about our passing, now the grammar is left recursive, you can see now the statement list, the statement list, statement case, so you can see on the right hand side, the statement list appears on the left most side, okay, then you have statement, okay, so the grammar actually is need more, more easy for human to understand, because every sin, you know, like if we have expression, and then you have operation in the middle, and then you have another operand, okay, so every sin is in one rule, not like the right recursive one, and so this rule will be broken down into two rules, sometimes even three ways, so one is the empty one, okay, so this is the rest of the grammar, okay, and then you can look at this basically is the, the stack is not shown here, but this is the the part of the statement which basically is used to, to the match in, and which is used to examine the the current, based on current state, and then current input to see what to decide, okay, what to do, okay, so in this particular diagram, only reduction actions are shown, the shift, the shift is not shown in this one, okay, so for example, your style is, stay zero, if you see R stands for read statement, right, so you read the statement, that's fine, okay, so you have read, and then you go to stay one, of course, it's condensed here, so you expect, you know, basically read some information there, and then if you see ID, now you expect a silent statement, then followed by a silent symbol, and then followed by some kind of expression, each stand for expression, and expression can have arithmetic addition operations there, may have some kind of multiplication operations there, and some also force, okay, so basically this give you a little peak of, you know, what does the state machine look like, which is used to control the actions of the overall, which don't, don't, okay, that's all for today, thank you.