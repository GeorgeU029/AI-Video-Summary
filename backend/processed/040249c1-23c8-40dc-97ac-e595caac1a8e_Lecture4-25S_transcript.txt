 In this lecture, we're going to discuss context free barmers and parsing. Context free barmers are essentially rules to define the syntax or programming languages. Thus, they are program generators. Parsers, on the other hand, push the automata. They recognize the language defined by context free barmers. Let's look at the context free grammar. Context free barmer can be defined by a set of four topos. We can look at the basic way. There's a set of terminals. We use t to stand for that. A set of non-terminoes. We use n to stand for that. A style symbol, a single symbol. Basically, s, which is non-termino. A set of production rules, where each rule is the form of x and the derive to w. The x is a non-termino. W can be a sequence of non-terminoes and terminals. Let's look at a simple example, and which is expression grammar. This grammar is defined by a typical arithmetic expression. We have five rules, actually, more than five rules, because some rules are combined. You can see we have expression, term, factor, add, operation, and multiplication operation. These non-terminoes appear on the left hand side of these rules. They have to be further replaced by the right hand side. Then we have term, occur on the right hand side of this rule, which is non-termino. Then we have this straight bar. We understand this is all, which means there's an additional rule here on the right hand side, which has expression. Addition operation and a term. It can see the second rule, basically, the non-termino expression appeared again. It appeared at the very left hand side. This grammar, if we define something recursively in terms of sub, and we call it out left-air recursion. If you remember, we talked about the regular expression where we can define repetition. We use clean start to define repetition. Where is additional power of context free grammar? Essentially, it's through the recursion. We have this recursion. You can define expression in terms of sub. This grammar is left-air recursive. Then we have these different rules. These different rules actually capture the precedence. That means which operation occurs before some other operations. In this example, you can see multiplication operations appeared at the different level from the addition operations. Actually, the way we arrange that, actually, the higher the level of rule actually has weaker precedence. Basically, that means at the level 5 here, these operations have high precedence. They will be derived first, or they will be used to combine the other things together first, then the rules above them. Not only we have precedence, also we have sensitivity. Basically, if we have left recursion, the grammar actually defines the left associativity. You combine things from left to right. Based on grammar, we can apply the rules to obtain derivation. Deirization essentially means how we generate the sequence. How we put the sequence into some kind of order according to grammar rules. One step derivation essentially is application of one rule by replacing the left-hand side, the non-terminal, with the right-hand side expression, which can be a combination of terminals and non-terminal. Deirization sequence consists of a sequence of these single-step derivations. We typically use this double-line error, plus means at least one rule, use the term zero or zero more steps. From these derivations, we can obtain the language defined by a grammar. The language defined by a grammar is a sequence of symbols, which can be derived from the start symbol, using one or more rules. Of course, this sequence needs to be all terminals. Non-terminal are used to further define some sense. They are not possible language, but terminals actually consist of the elements of a language. We understand the language, of course, the grammar defined the language. Language can have many, many sentences of these sequences. For each sequence, we can use a pass-tree to represent that. For each sequence, we can have a derivation. Of course, this sequence needs to be a valid sentence in the language. If it's a valid, then we can derive a pass-tree to represent this derivation. Here are some examples for the arithmetic expression grammar. In this particular case, we have three plus four multiply five. You can see we have this pass-tree based on grammar rules. For every application, a grammar rule basically adds one level to the tree. For example, for example, from expression, there is a rule which consists of expression, add, operation, and a term. We apply a one rule, we generate one level, and then from expression to term, you see one derivation rule, we generate another level, and a so on and so forth until we reach the terminal, which is numerical number three. Here, we have addition operation and a so on and so forth. That's right. Two of course. Two of course. Two of a gamblers. You can see here the multiplication takes precedence over addition because here you have to actually calculate this branch first. To reach the term, then this term will be used in the computation of this addition operation. Here is another example which shows the left associativity. Basically, you can see since the grammar is left recursive, so the passing will generate this kind of tree, which basically says we compute 10 minus 4 first, and then followed by minus 3. This branch is lower, so that means it needs to be completed or reduced first before we can get the value, and which can be used to compute the overall expression. We have seen one good grammar, which is basically left recursive, refracts the precedence and associativity. Of course, not other grammar possess that kind of attributes. We will see some grammar and big use. It's important to understand the grammar when we say grammar is ambiguous. Basically, you can find some string or some sentence, which can produce two past trees, which have different structure. In this particular example, let's say we have this grammar, basically E expression grammar, we have some identified I, then we have addition, subtraction, multiplication, division, and so on and so forth. All these rules are at the same level. If they are at the same level, that means you cannot reflect the precedence of the operation. You end up with this particular expression. You can generate these two past trees. The other way is that it is basically the operator precedence in refracting this grammar. We talk about different levels. Also, associativity, because this grammar tends to be both left recursive and the right recursive. You see E appeared at the leftmost side as well as rightmost side. You can either have left recursion or you can have right recursion, so that causes a problem. If the grammar is ambiguous, in most cases, we can turn that grammar into non-amiiguse grammar. You need to be aware when we say the grammar is ambiguous, basically we can't have two past trees. But the grammar is ambiguous, doesn't mean the language is ambiguous. If the language ambiguous are there, no matter how they try, you cannot get ambiguous grammar. These are two different concepts to make sure you understand that. Typically, if the grammar is ambiguous, we can introduce precedence as well as associativity to resolve the problem. Essentially, we use a different non-terminal for each precedence level. We always start a rule with the upper, with lowest precedence, and then a rule for the upper, with the next, lowest precedence, and so on. That is basic idea. If we want the left associativity, we can use left recursion. If we want the right associativity, then we can use right recursion. The previous grammar can be changed now to this grammar, and which is left recursive. The left associativity, at the same time, has the precedence. The highest precedence is reflected by the last rule, and then move upwards, we have lowest precedence. Think about when you have the rules at the top, you can see the derivation will appear at the high level of the tree. However, in terms of computation, what happens is basically you go from the leaves. Then you go gradually towards the root. That means the root basically has low precedence. That means what happened at the time of dealing with elements at the root will be much later, what happens when we deal with the leaves. Now we have some idea about what is context grammar. How can we define language? Once we have context free grammar, we need to do passing. Basically, we try to do the other way around to recognize the language defined by context free grammar. Basically, we try to define a generalize the parsers for the language. Of course, there can be many different grammars for every context-free language. So you can generally own grammars basically or eventually define the same language, but not all grammars are created equal. That means some are better in terms of processing they are more efficient or maybe more a do-standard. Okay. Some grammars are probably not that good, for example. And big use can. So how we can build a passers essentially in a different way is we know we can create a passer which runs in a big old notation. I guess some of you may know this. Some of you may not know this. Okay, big old essentially means upper bound. Okay. And so the time bound basically is here and cube and stand for the size of input. Okay, so it's a polynomial. And it runs okay, but it's not fast enough. We'll see some special average special class for grammars which can be more efficient. In most cases that will be much more desirable. So there are two well-known passing algorithms which can achieve this kind of performance and based on their inventors when it's called the earliest algorithm algorithm is cookie younger Kassami algorithms. Okay. And so these are well-known algorithms, but they are not very efficient. So how can we get more efficient grammars? Okay, essentially we look at some class. Okay, so we look at what can be defined using the compact grammar and which can produce very efficient passers. So there are two major classes. Okay, and one is called LL. And first L stands for left to right. Okay, so you scan for left to right. Second L basically means the left to most derivation. Okay, so you need to look at left most non-terminal then you replace that with the right side expression. Okay, so the derivation always tackles left most non-terminal. These passers also call the top down are predictive passers. Okay. So you derive from top because you try to predict something. Okay, so you build a pass tree from the root to the leaves, essentially. Okay. And one thing you need to understand is when we talk about LL also, L is in a most stand for left to right. Which are actually applicable to right to the recursive grammar. It's not left to recursive grammar. Okay, so I always find students get confused about this. Okay, in my previous offering of this class throughout many years. Okay, so make sure you understand that. Okay, so why it's left to recursive grammar is not suitable for left to most derivation. Okay, and one thing is obvious is basically if you have left to recursion, if you try to replace it, you will replace again, again, again, infinitely monetize because there's no end, right? To to that. So that's why when the grammar is left to recursive, so you basically cannot use left to most derivation. So you have to use right most derivation. Okay, so that's exactly what this passing technique is for. Okay, so LR techniques are for. Okay, so it's for right most derivation process. Okay, which are bottom up of shifted reduce. Okay, so that means they build the pass tree from bottom up. And this type of passing techniques are applicable to the left recursive grammar. Okay, because you are handling the left to recursion at the last moment when you got every information you need. Okay, from the right side. Okay, so that's why once you have that, you can do reduction. Okay, so that is the major idea. So you need to understand. Okay, so the left recursive grammar is only a property for LR passing. If it's the right recursive grammar, we can use LL passing techniques. Okay, so for LR passing, there are also special classes like SLR, which is simple LR and then look ahead LR techniques. Okay, so that's why you need to understand. And often when you see LL or there's a number in the parentheses of update. Okay, and so this number essentially is used to indicate how many tokens you need to look ahead. Okay, so if it's one, so besides the token, when you are processing at the moment, you need to look ahead when more token, and after this one. Okay, so to make a decision. Okay, so that is important. So that number itself gives you some idea, but how many characters we need to look ahead. Okay, so in most cases, this look ahead by one token is adequate, but in some case may not be. Okay, so it's very similar to the regular expression where when we try to build tokens from the character sequence, we also need to look ahead to the characters, but here we look ahead for the tokens because now we have more meaning for building blocks for the language. So this basically tells you something about the relationship between different grammars. Okay, so LL1 grammar is also LR1 grammar. Also the right recursion will cause problems for LL1 grammar. So it just discussed the basically or create the very deep stack. Okay, so you can't predict repeatedly using the same rule, okay, essentially. Okay, and for every contest free grammar, you can pass deterministically and which has SL1 grammar. Okay, of course, which is subset, simple version of LR. So if you can pass contest free grammar, which has something called the prefix property, it's a term is no valid string is a prefix of another valid string. If the grammar has this property, then you don't even need to look ahead because everything you see is unique. So there's no ambiguity. So there's no reason. I don't need for you to look ahead to determine and resolve the problem. Because itself provides adequate information for you to make a decision. So let's look at some more meaningful examples here. Okay, so this is a grammar, context free grammar for the LL1 passer, for the LL1 passer passing technique. So in this case, we have a program, which is a statement list, a statement list, is statement followed by a statement list and each statement can be as a ness in there, or assignment, a read statement, write statement. And then for the assignment statement, do you have expression? You further define what are the expressions and so on and so forth, that a few more rules in the next page. Okay, if you pay attention to this grammar, you can easily see this grammar as write recursive because you see the statement list appears again on the right hand side, but this is the right most, right? So it's right recursive as we already discussed right recursive grammar in suitable for LL passing. And again, you will see here, turn tail, turn tail basically appears on the right most. So it's right recursive grammar and it's suitable for LL passing. So these are the remaining rules of this grammar. So like the bottom up grammar, this one captures the associativity and precedence. And however, it doesn't look very neat. Okay, so for one reason, let's quickly go back. See, basically here, if we look at the fact, factorial factor tail in this case, and which star with multiplication are placed, with a factor and a factor tail, and then it has another rule which is empty. So you can't think about the basically for us, when you have multiplication, multiplication operation, the operator is in the middle, right? You have something on the left, something on the right, then you have operation in the middle. But in this grammar, the operation is actually defined in the front and the other part basically now is in the previous rule. Okay, so you see you basically break the expression is in two rules. Okay, so you have factor, factor tail, which involves operator and another operand here. So it's not natural for human to understand. However, this kind of grammar, right recursive grammar is very nice for LL patch. So basically you can produce a very simple passers. So to handle this kind of thing. Okay. And we do that, build a pass tree by doing it from incremental approach. So let's look at the concrete example. Now we have that grammar. Okay, now we can have a program defined using that grammar. And then we can see how we can build a pass tree from this program. Okay, so make sure you understand this. Okay, also a concept. Okay, so when we have a grammar, we don't have a pass tree. Okay, so grammar is generic, right? A grammar defines essentially infinite many strings or sentences. So when we talk of pass tree, it is only associated with a particular string, a particular program. Okay, so you have a programming language, you can write any program you can imagine. So, but each program can be passed to generate pass tree. And then to generate code eventually. Okay, so you don't generate code for language. Because that doesn't make sense. So you generate code for a program written in that language. Okay. So now let's look at this particular program. So we can build a pass tree. Okay, for this. Okay, and here's the pass tree, essentially using those 19 rules, we just saw basically using the LL pass in technique. So you can see basically we start with, it's a top down right, we start with a program. And we put that into the stack, essentially, you know, we have pushed on a tomat, right? With that into the stack, then we match the read, because read is first statement, right? So match the read, now we push the statement list in, and then we do that again again, which tells you exactly how we do that. We basically have a table or see a table in the next slide. Okay. So what happens in passing, essentially have a big loop. Okay, so you repeatedly look up for an action two dimensional table based on the current left most non-terminal case in front of us, because left most derivation, we always focus on the left most non-terminal with the current input token, we might need to look when more token had. Okay, so what are actions? What does the pass do? Okay, so the pass is, you know, not only do that, but also, you know, search for something at the same time make decisions, and either match a terminal, so if you match a terminal, you pop the stack, okay? If you, so that's one action, okay? And of course, once you pop a stack, you can directly build that into the tree, or you can save it somewhere then you build a tree at the very end, okay? Or you predict a production, okay? So, so again, when you predict the production here, you basically pop the non-terminal in the stack, and there you push the right side of the rule into the stack. Okay, so if none of those, for example, you cannot match a terminal, or you cannot find a production rule to predict, in this case, basically the syntax is wrong, so you have to produce error message, okay? So in this case, so here's a table actually, which tells you what to do, okay? So, so let's say this is the top of stack, you have a non-terminal here, and this table only refracts the prediction product, because the matching product is trivial. So you have program, now if this is on top of a stack, and then if you see the ID here, and you apply production rule, what? Okay, so basically which tells you now you have a statement list, you have to pop, you have to push down the statement list, and so if you have statement list, you see ID, not basically use rule two, which basically says, it's a statement list and a statement, okay? So also force, okay? So if you want to check the details, you can go back a few slides, and match that with the rules, okay? So these are the rule numbers here, and which tells you what to push to the stack, essentially replace the top element of the stack, where is the right hand side expression, and push those onto the stack, okay? Keep doing that, that's for prediction, okay, these are all tell you prediction, but if you match the terminal, which are not shown here, then you just pop up the terminal, and so whatever you pop up, these need to be saved, was it's terminal, non-terminal, and eventually these will be used to build a pass tree, of course you can build a pass tree at the same time, it basically it's a strategy, whatever you can do, and it doesn't make any difference. So as I just mentioned, to keep track the level most, non-terminal you push some scene, you have not seen, at the right side expression of a rule onto the stack, okay? And you can see a little more details in the textbook, in this figure, and so it is important to remember basically the stack will contain all the stuff you expect to see between now and end of program, okay? Of course, not at once, but dynamically, okay? So you add a some scene, and you pop up some scene, you push down some scene, pop up some scene, whenever you pop up, basically I used to build this derivation tree or pass tree, okay? Okay, so what happens if we want to do one pass in, but the given grammar is not, given grammar is not recursive, it's left recursive, okay? So what we'll have to do is first we have to convert this grammar into equivalent right recursive grammar, and then we can't plot LL1 passing, okay? So in this case, so for example, in this case, if we have ID list, goes to ID list, ID list, followed by ID, okay? So you see you can have a sequence of IDs here, but this rule itself is left recursive, okay? Because ID list in the second rule appears on the leftmost side, okay? So it's left recursive, if it's left recursive, it is not suitable for LL passing, okay? So in this case, so however we can basically mechanically translate that into ID list, ID list tail, and then ID list tail, that was the comma, and then followed by ID, and followed by ID list tail, all empty, okay? So now one rule in the left recursion is translate into essentially three rules, right? One, two, and three, the last one is a blank, okay? And a quiche now is right recursive, okay? So you have ID list tail, ID list tail, which appears on the rightmost side of the rule, okay? And so by doing that, now you can apply the LL passing, to solve the problem, okay? To successfully generate the pass tree. So one thing is how we can turn the left recursive grammar into right recursive graph, okay? And another technique is to deal with common prefix, okay? So basically if we have to look more than one, token in this case, then we have trouble, right? So we cannot determine, okay? So we can only look at at the most of what. So the reason we need to look ahead is because you have the same token, but then a token behind it can determine different decisions in this case. So it's not unique. So in order to make sure our grammar can process that by looking just the one token head, so we can do something called a left factory, okay? Here's example, okay? So we have statement, I, okay, a statement is defined by a assignment statement or a function call, okay? So you see ID here, followed by assignment operation and ID here, followed by a left process, okay? So if you just look at ID, you cannot tell what to do, okay, whether it's assignment or whether it's a function call. You have to look at one token head, right? So we can resolve this problem by left the factoring. So we have statement ID, then we have ID statement tell. So in ID statement tell, now we can have whatever remaining part of this rule, we put here and a remaining part of this rule, we put behind this, okay? So now, so everything is unique, okay? So we basically process that, we can do a derivation and push this ID statement tell onto the stack and then whenever the prime ID statement tell a meet with this assignment operation, then and no, basically it's assignment statement where it sees the left, press, and it knows it's a function call, okay? So this can be done again mechanically, okay? So we basically can transform a grammar by left the factoring to make sure, you know, the non-uniqueness thing is resolved, okay? So I think in our whole assignment, you will have a question related to this. So make sure if you have questions, you can go back to this and just get some concept and if you still have doubt, you can read a little more to understand that. Okay. However, okay, so these are the common problems. If we, let's say we animate left recursion or common prefix and it may not always work, okay? So that depend on whether the language itself is is non-LL or not, okay? So essentially that infinite many language which are not no, okay? So no matter what you do, you know, resolve left recursion, common prefix, you still can pass them using LL techniques, okay? So, but in most cases, you know, we don't have that common problem. If we have, in some languages, if we have that problem, we can't always, you know, use some special tricks to resolve them. Okay, so here are some examples in how a language may not be LL1. And so one famous example is called a dangling else, okay? So, so in this case, okay, it doesn't matter how many tokens you look ahead or you cannot still determine how you do it, okay? And of course, this actually appeared in some real programming language, you know, Pascal, it's a well-known language. And the reason it's ambiguous, that means you cannot make this, okay? So let's look at what does this mean exactly? Okay, so you have a statement which is if statement, okay? So we understand a keyword if, or by some kind of condition, then we have a dang class or else class, okay? And then you can have other statement, you know? But let's focus on this if statement, okay? So the dang class can have a dang and then followed by statement. The else class now has else and statement, okay? So that's not problem. But it can also have an option called epsilon, okay? Absalom basically here means empty, right? So if the else part is empty, that class problem because we can't have if then some statement, okay? So let's say another if, okay? So if condition, then if condition, then statement, else statement, okay? So we don't know that else statement should appear so it's the first then, the second thing, right? So that class confusion because the grammar allows that, okay? So there's, you know, maybe intentionally when somebody write a program, they may not aware, and exactly that will cause problem for the compiler to determine, okay? So all we can do is basically we can always, the always this kind of problem by, you know, say using some special rules, okay? Also the grammar is ambiguous, okay? But we can always, you know, treat some special rules to the always that. For example, we, you know, always, you know, pair the else to the closest to them, okay? So in this case, of course, the second is then, so there is an ambiguity. So more generally, we can have two rules, okay? So basically there are two rules, right? Then have one, the else, you know, which has the else part, the empty one, okay? So we can always use, say, the first one to use. So in this case, again, we look at the else, we always pair that with the second thing, okay? Because we use the first rule in the else part, else class, okay? Which is the else statement, not the the absinal, not the empty one, okay? So this is how we deal with some intentional design for us in the language. But you know, in some modern languages, okay? So this problem can be resolved by just to introduce these explicit delimitators for the statements. For example, you try to always have an end to close a statement. And of course, in some languages, not necessarily using and these, you know, if or in an FYK. Okay, so the reverse spelling of if, okay, to signify that's the end of this particular statement, if you use that, so you will never have an ambiguity occur in your program. So we understand it now. What does our pass it mean? And basically it has push dot, tomat, then you have these rules, you do prediction, then you use to generate pass string, okay? So actually how we can actually build these, you know, predict the set, okay? So basically say, you know, build that table, right? We have that table and predict what to use. And so we can use something one known basically calculate the sum, use these functions, got a first, for all, and to predict, compute these to for the symbols, okay, which include nonterminal term symbols, and then follow just for nonterminal symbols, okay? So these function on one, and which are defined by these rules, essentially, and a first of this rule alpha, okay, you focus on this left hand side. Basically says if you have a sequence of derivation, it's the first, whatever, nonterminal, terminal, okay, A and E, collected, it's the first appear, there you can appear in the first, the derivation, and then if you have this case, so follow up case, so in this rule, it's from the star symbol, okay, so you can have a sequence of alpha, okay, so some strings there, some tokens there, and so what follows this A is again, this A, okay, the sum of it, after capital A, okay? So you can define this, and then of course, you can use the predictor set calculate and create the combination of first and follow, okay? So these are one, no, you don't need to actually look at details, but if you want to look at details, these are computed actually, they're quite straightforward, and here are some examples from that language we just saw, and for example, I just mentioned a few things, okay, so let's say you have a statement, statement list, okay, so in this case, what follows the statement list, okay, so the first in the statement list, basically you only have, have basically the empty, right, empty symbol, okay, so no other IDs are the terminal, okay? But if you look at the first in the statement, okay, so first in the statement here, basically in the assignment statement, you have ID, okay, so that appear the first in this statement, okay? And similarly in this statement, the first in the read, and in this statement, the first is right, okay, so we have some idea of a first, and let's look at the way to follow, okay? So what follows ID here, what follows ID is this assignment, symbol, right? And in this statement, what follows ID, what follows the read is ID, okay? So these are actually fairly straightforward, and easy to understand, okay? Of course, the detailed computation is a little involved, but first these progress, you know, they are for machine to process enough for human to read. And this is the continuation, so, you know, these actually rules are all built by these functions, okay? So you predict it will generate this set of rules and find out what's in verse, you know, set, you know, first, okay, follow and predict, and so on, so forth, okay? So when we do our passing, if an token belongs to the predict the set of more than one production, with the same left hand side, then the grammar is not LL1, okay? Of course, in this case, it's very easy to understand, okay? So if you have some token belong to the predict set, of one more production in this case, and then you don't know which production to use, right? Because it costs ambiguous. A conflict can also arise because same token belong begin more than one right hand side, or as a case, you know, it can also appear after left hand side in some valid program. And for example, like an absolute, okay, the empty one, okay? So these are just some general rules of the apply those functions to compute these, and you gather the information, then you can do a check and I see what's in the grammar, actually, it's suitable for LL1 passing or not. Okay, so we have some idea about LL passing, okay? So the other one we're going to briefly mention is our passing, okay? So this is pretty much table driven, okay? So very similar to the LL passer, but the LR passer actually is a little more complex, okay? So it's still is a loop, you know, keep looking for the next reduction, okay? Application reduction rule. And in this case, it's inspect the two dimensional table to find what action to take, just like the LL passer, we saw tables there, okay? However, unlike the LL passer, the LR driver has a non-trivial state machine, okay? So basically in the, I'll passer, it's very simple. In this case, because you just do prediction, prediction, okay, you don't, you don't need to keep memorizing things, okay? And in the LR passer actually, so you need to have the term is finite state, at least to keep track of where you are at this moment, okay? So because it's in some sense, it's a bottom up, so you don't have the overall picture, okay? So you need to know the context, okay? Which place currently is being reduced, okay? So this is indexed by the current input token and current state, okay? So it's a little more complex than the finite state machine we discussed during the scanning. So where we only need to have the input character, but here we need to have token and a current state, okay? So we need both, okay? And so state contains a record of what has been seen so far, okay? Not what is expected, okay? So it's very different, okay? So here what's in the stack is what you have already seen so far, okay? And it's not like the stack in the L-passion, in the L-passion where we put something we haven't seen yet, okay? So this is a little bit different, okay? So make sure and understand that. What's in the stack is different because when it's top down, when it's bottom up, okay? So we save different type of information. So we need to have a look at the stack, we save different type of information. So basically push dot, how much can be defined as a state diagram, okay? So we have basically a state machine and a stack, okay? So we, you know, push dot, how much is basically, it's a deterministic state machine plus a stack, okay? So that's why you see the power of push dot, atomic, which is, you know, deterministic finite state machine plus stack. So you need to have this additional mechanism, storage mechanism, okay? So there is no difference essentially when we look at the state diagram of deterministic finite state, atomic, okay? Except the arcs are now labeled, the way input symbol as well as the top of the stack symbol, okay? So you need to not only look at the input token, which you're considering right now, but you also need to look at the top of the stack, okay? So what, what, what do you try to do? You know, do at this moment, okay? So, so there are a pair of input, okay? So in this case, and then you make a decision, either you push in pop and, you know, for the symbols onto of the stack, actually, you can do more than what you can push in sometimes because you need to push a bunch of things into, or you can't pop a bunch of things when you do the reduction, you will pop multiple symbols to, to satisfy a rule. When you push, it could also push more than one symbol into the stack. So we just mentioned that the LL also is pushed down atomic, but in this case, the state machine itself is very simple, okay? So we really don't need to remember much because everything can, can be inferred from the stack. So the stack actually contains more information we need because we do the leftmost case, so we always, you know, do whatever we can do and do a reduction, okay? So, so you don't, the context in this case is much smaller, okay? So you always do the current thing, you can make a decision and do a reduction, okay? So you can process that, okay? However, when you do LR, so you have to basically push a lot of things into the stack, then you know, because the rightmost, right? Rightmost means, you know, if you don't see it yet, everything need to be saved on the stack, then when you reach the rightmost, non terminal, then you can do the reduction, but by that time, lots of things are already on stack, so now you need to figure out, you know, how much you need to process, you know, at a time, okay? But when you have LR, basically the leftmost case, so you just look at the current top of stack and whatever the input that you make a reduction, in that case, you make a prediction, okay? So typically, you can imagine, you know, the push dot atomic in the LLL passing is shallow, that means you don't need to say much on to the stack, but in LR passing, you have to essentially save almost everything into the stack, the stack will be very big in the very beginning, because you need to see the rightmost to one, okay? And then you start to reduction, okay? So you need to, you know, normally it has a deeper stack, okay, when you do LR passing, okay? So builds, basically, are a PDA, have multiple states, okay? So builds the pass tree from bottom up, okay? And you need to keep track, which production might be in the middle, that's what I just mentioned in the middle, okay? So you need to aware a larger context in this case, okay? So this, this same machine we call characteristic, okay, finite state machine is used based on shift and a reduce, basically, you know, you use that, the state decide, you know, was it, we should shift or was we should reduce, okay? And then we have the same language, but defined by a slightly different grammar, of course, you know, when we talk about our pass, now the grammar is left recursive, you can see now the statement list, the statement list statement, okay, so you can see on the right hand side, the statement list appears on the leftmost side, okay? Then you have statement, okay? So the grammar actually is neat, more, more easy for human to understand, because every sin, you know, like if we have expression, and then you have operation in the middle, and then you have another operand, okay? So every sin is in one rule, not like the rye recursive one, and so this rule will be broken down into two rules, sometimes even three, so one is the empty one. Okay, so this is the rest of the grammar, okay? And then you can look at this basically is the, the stack is not shown here, but this is the part of the statement, which basically is used to the matchin, and which is used to examine the current, based on current state, and then current input to see what to decide, okay, what to do, okay? So in this particular diagram, only reduction actions are shown, the shift, the shift is not shown in this one, okay? So for example, your style is stay zero, if you see r stands for read statement, right? So you read a statement, that's fine, okay? So you have read, and then you go to stay one, of course, it's condensed here, so you will expect, you know, basically read some information there, and then if you see ID, now you expect a silent statement, then followed by a silent symbol, and then followed by some kind of expression, E stands for expression, and expression can have a rhythmic addition operations there, may have some kind of multiplication operations there, and some also force, okay? So basically this gives you a little peak of, you know, what does the stay machine look like, which is used to control the actions of the overall push down the tone? Okay, that's all for today, thank you.