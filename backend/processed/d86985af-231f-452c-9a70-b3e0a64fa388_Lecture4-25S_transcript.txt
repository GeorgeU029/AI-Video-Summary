 In this lecture, we're going to discuss context free barmers and parsing. Context free barmers are essentially rules to define the syntax or programming languages. Thus, they are program generators. Parsers, on the other hand, push the automata. They recognize the language defined by context free barmers. Let's look at the context free grammar. Context free barmer can be defined by a set of four topos. We can look at the basic way. There's a set of terminals. We use t to stand for that. A set of non-terminoes. We use n to stand for that. A style symbol, a single symbol. Basically, s, which is non-termino. A set of production rules, where each rule is the form of x and the derive to w. The x is a non-termino. W can be a sequence of non-terminoes and terminals. Let's look at a simple example, and which is expression grammar. This grammar is defined by a typical arithmetic expression. We have five rules, actually, more than five rules, because some rules are combined. You can see we have expression, term, factor, add, operation, and multiplication operation. These non-terminoes appear on the left hand side of these rules. They have to be further replaced by the right hand side. Then we have term, occur on the right hand side of this rule, which is non-termino. Then we have this straight bar. We understand this is all, which means there's an additional rule here on the right hand side, which has expression. Addition operation and a term. It can see the second rule, basically, the non-termino expression appeared again. It appeared at the very left hand side. This grammar, if we define something recursively in terms of sub, and we call it out left-air recursion. If you remember, we talked about the regular expression where we can define repetition. We use clean start to define repetition. Where is additional power of context free grammar? Essentially, it's through the recursion. We have this recursion. You can define expression in terms of sub. This grammar is left-air recursive. Then we have these different rules. These different rules actually capture the precedence. That means which operation occurs before some other operations. In this example, you can see multiplication operations appeared at the different level from the addition operations. Actually, the way we arrange that, actually, the higher the level of rule actually has weaker precedence. Basically, that means at the level 5 here, these operations have high precedence. They will be derived first, or they will be used to combine the other things together first, then the rules above them. Not only we have precedence, also we have sensitivity. Basically, if we have left recursion, the grammar actually defines the left associativity. You combine things from left to right. Based on grammar, we can apply the rules to obtain derivation. Deirization essentially means how we generate the sequence. How we put the sequence into some kind of order according to grammar rules. One step derivation essentially is application of one rule by replacing the left-hand side, the non-terminal, with the right-hand side expression, which can be a combination of terminals and non-terminal. Deirization sequence consists of a sequence of these single-step derivations. We typically use this double-line error, plus means at least one rule, use the term zero or zero more steps. From these derivations, we can obtain the language defined by a grammar. The language defined by a grammar is a sequence of symbols, which can be derived from the start symbol, using one or more rules. Of course, this sequence needs to be all terminals. Non-terminal are used to further define some sense. They are not possible language, but terminals actually consist of the elements of a language. We understand the language, of course, the grammar defined the language. Language can have many, many sentences of these sequences. For each sequence, we can use a pass-tree to represent that. For each sequence, we can have a derivation. Of course, this sequence needs to be a valid sentence in the language. If it's a valid, then we can derive a pass-tree to represent this derivation. Here are some examples for the arithmetic expression grammar. In this particular case, we have three plus four multiply five. You can see we have this pass-tree based on grammar rules. For every application, a grammar rule basically adds one level to the tree. For example, for example, from expression, there is a rule which consists of expression, add, operation, and a term. We apply a one rule, we generate one level, and then from expression to term, you see one derivation rule, we generate another level, and a so on and so forth until we reach the terminal, which is numerical number three. Here, we have addition operation and a so on and so forth, and the precedence is refracted by the level of this tree. You can see here the multiplication takes precedence over addition, because here you have to actually calculate this branch first. To reach the term, then this term will be used in the computation of this addition operation. Here is another example which shows the left associativity, and basically you can see since the grammar is left recursive. The passing will generate this kind of tree, which basically says we compute 10 minus 4 first, and then followed by minus 3, because this branch is lower. That means it needs to be completed or reduced first before we can get the value, and which can be used to compute the overall expression. We have seen one good grammar, which is basically left recursive, refracts the precedence and associativity, and of course not other grammar possess that kind of attributes. We will see some grammar and big use. It is important to understand the grammar when we say grammar is ambiguous. Basically you can find some string or some sentence, which can produce two past trees, which have different structure. In this particular example, let's say we have this grammar, basically the expression grammar, we have some identified I, then we have addition, subtraction, multiplication, division, and so on and so forth. All these rules are at the same level. If they are at the same level, that means you cannot reflect that kind of precedence of the operation. So you end up for this particular expression, you can generate these two past trees. The other way is that basically the operator precedence in refracting this grammar. We talk about different levels. Also, associativity, because this grammar tends to be both left recursive and a right recursive. You see, it appears that at the leftmost side, as well as rightmost side. You can either have left recursion or you can have right recursion, so that constant problem. But if the grammar is ambiguous, in most cases we can turn that grammar into non-ambiguous grammar. But you need to be aware when we say the grammar is ambiguous, basically we can't have two past trees. But the grammar is ambiguous, doesn't mean the language is ambiguous. No matter how they try, you cannot get ambiguous grammar. These are two different concepts to make sure you understand that. Typically, if the grammar is ambiguous, we can introduce precedence as well as associativity to resolve the problem to make the grammar ambiguous. Essentially, we use a different non-terminal for each precedence level. We always start a rule with the operator with lowest precedence and then a rule for the operator with the next lowest precedence and so on and so forth. That is basic idea. If we want the left associativity, we can use left recursion. If we want the right associativity, then we can use right recursion. Now, the previous grammar can be changed now to this grammar, which is left recursive. Now, the left associativity has the precedence level. The high precedence is reflected by the last rule. Then move upwards. We have the lowest precedence. And you can't think about when you have the rules at the top, you can see the derivation will appear at the high level of the tree. And however, in terms of computation, what happens is basically go from the leaves. Okay, then you go gradually towards the root. Okay. So that means the root basically has low precedence. That means what happened, you know, at the time of dealing with elements at the root will be much later, then what happens when we deal with the leaves. No, it's okay. Okay, so now we have some idea about what is context grammar. And so how can we define language? Now, once we have context free grammar, we need to do the passing. So basically try to do the other way around to try to recognize the language defined by context free grammar. Okay. So basically we try to define a generalize the passers for the language. And of course there can be many different grammars for every context free language. Okay, so you can, you know, generally own grammars basically, well, eventually define the same language, but not all grammars are created equal. That means some are better in terms of, you know, processing. They are more efficient or maybe more do a stumble. Okay. Some grammars are probably not that good, for example, and big use. Okay. So how we can build a passers essentially, in a different way, we know we can create a passers, which runs in big old notation. I guess some of you may know this, some of you may not know this. Okay, big old essentially means upper bound. Okay. And so the time bound basically is here and cube and stand for the size of input. Okay. So it's a polynomial. And it runs okay, but it's not fast enough. We'll see some special average special class for grammars, which can be more efficient in most cases that will be much more desirable. So there are two well known passing algorithms, which can achieve this kind of performance. And based on their inventors, when it's called the earliest algorithm, other is cookie younger, Kassami algorithms. Okay. And so these are well known algorithms, but they are not very efficient. So how can we get more efficient to grammars? Okay. Essentially, we look at some some classes. Okay. We look at, you know, what can be defined using a very compound grammar, and which can produce very efficient passers. Okay. So there are two major classes. Okay. And one is called LL. And first L stands for left to right. Okay. So you scan for left to right. Second L basically means the left the most derivation. Okay. So you need to look at left most non terminal. There you replace that with the right side expression. Okay. So the derivation always tackles left most non terminal. These passes also call the top down or predictive passers. Okay. So you derive from top because you try to predict something. Okay. So you build a pass tree from the root to the leaves, essentially. Okay. And one thing you need to understand is when we talk about LL also, else, you know, most stands for left to right, which are actually applicable to right to the recursive grammar. It's not left to recursive grammar. Okay. So I always found a student to, you know, get confused about this. Okay. And my previous offering of this class throughout, you know, many, many years. Okay. So make sure you understand that. Okay. So why it's left the recursive grammar is not suitable for left to most derivation. Okay. And one thing is obvious is, is basically, if you have left the recursion, if you try to replace it, you will replace it again, again, again, infim infinitely, many times because there's no end, right? To, towards that. So that's why when the grammar is left to recursive, so you basically cannot use left the most derivation. So you have to use right most derivation. Okay. So that's exactly what this passing technique is for. Okay. So LR techniques are for. Okay. So it's for right most derivation process. Okay. Which are bottom up of shift the reduce. Okay. So that means they build up the pass free from bottom up. And this type of passing techniques are applicable to the left recursive grammar. Okay. Because you are handling the left recursion at the last moment when you got every information you need. Okay. From the right side. Okay. So that's why once you have that, you can do reduction. Okay. So that is the major idea. So you need to understand. Okay. So the left recursive grammar is the right. The grammar is only a property for LR passing. If it's the right recursive grammar, we can use LL passing techniques. Okay. So for LR passing, there are also special classes like SLR, which is simple LR. And then look ahead LR techniques. And often when you see LL or there's a number in the parentheses of update. Okay. And so this number essentially is used to indicate how many tokens you need to look ahead. Okay. So one, so besides the token you are processing at the moment, you need to look ahead when more token. And after this one. Okay. So to make a decision. Okay. So that is important. So that number itself. Give you some idea. But how many characters we need to look ahead. So in most cases, this look ahead by one token is adequate. But in some case may not be. Okay. So it's very similar to the regular expression where when we try to build the tokens from the character sequence, we also need to look ahead to the characters. But here we look ahead for the tokens. Because now we have more meaning for building blocks for the language. So this basically tells you something about the relationship between different grammars. Okay. So LL1 grammar is also LR1 grammar. Although the right recursion will cause problems for LL1 grammar. So it just discussed the base gate will create a very deep stack. Okay. So you can't predict it repeatedly using the same rule. Okay. Essentially. Okay. And for every contest programmer, you can pass deterministically. And which has SL1 grammar. Okay. Of course, which is a subset, a simple version of LR. So if you can pass a contest three grammar, which has something called the prefix property, it certainly means no value. The string is a prefix of another value string. If the grammar has this property, then you don't even need to look ahead. Because everything you see is unique. So there's no ambiguity. So there's no reason. No need for you to look ahead to determine and resolve the problem. Because itself provides adequate information for you to make a decision. So let's look at some more meaningful examples here. Okay. So this is a grammar, context free grammar for the LL1 passer. A passing technique. Okay. So in this case, we have a program, which is a statement list. A statement list is statement followed by a statement list. And each statement can be as a mess in there are assignment read statement, write statement. And then for the assignment statement, do you have expression? You further define what are the expressions and a source of force. And then you can see that a few models in the next page. If you pay attention to this grammar, you can easily see this grammar as right recursive. Because you see the statement list appears again on the right hand side. But this is the right most right. So it's right recursive as we already discussed the right recursive grammar. It's suitable for LL passing. And again, you will see here, turn tail, turn tail basically appears on the right most. So it's right recursive grammar. And it's suitable for LL passing. So these are the remaining rows of this grammar. So like the bottom up grammar, this one captures the associativity and precedence. And however, it doesn't look very neat. So for one reason, let's quickly go back. See basically here, if we look at the factorian factor tail in this case, which studies multiplication operation with a factor and a factor tail. And then it has another rule which is empty. So you can't think about the basically for us, when you have multiplication, multiplication operation, the operator is in the middle. You have something on the left, something on the right, then you have operation in the middle. But in this grammar, the operation is actually defined in the front. And the other part basically now is in the previous rule. So you see you basically break the expression is in two rules. So you have factor, factor tail, which involves operator and another operand here. So it's not natural for human to understand. However, this kind of grammar, recursive grammar is very nice for LL passing. So basically, you can produce very simple passers. So to handle this kind of thing. And we do that, build a pass tree by doing it from incremental approach. So let's look at the concrete example. Now we have that grammar. Now we can have a program defined using that grammar. And then we can see how we can build a pass tree from this program. So make sure you understand this concept. So when we have a grammar, we don't have a pass tree. So grammar is generic, right? A grammar defines essentially infinite many strings or sentences. So when we talk of pass tree, it is only associated with a particular string, a particular program. So you have a programming language. You can write any program you can imagine. So each program can be passed to generate pass tree and then to generate code eventually. So you don't generate code for language. Because that doesn't make sense. So you generate code for a program written in that language. So now let's look at this particular program so we can build a pass tree for this. And here's the pass tree essentially using those 19 rules we just saw basically using the LL pass and technique. So you can see basically we start with it's a top down right. We start with a program and we put that into the stack essentially, you know, we have pushed on a Thomas, right? With that into the stack, then we match the read because read is the first statement, right? So match the read. Now we push the statement list in and then we do that again again, which tells you exactly how we do that. We basically have a table or see a table in the next slide. So what happens in passing essentially have a big loop. So you repeatedly look up for an action to dimensional table based on the current left most non terminal case in front always because left most derivation we always focus on the left most non terminal with the current input token we might need to look when more token had. So what actions what the passers do, so the passers not only do that, but also search for something at the same time make decisions and either match terminal. So if you match terminal, you pop the stack. So that's one action. And of course, once you pop a stack, you can directly build that into the tree or you can save it somewhere then you build a tree at the very end. All you predict a production. So again, when you predict the production here, you basically pop the non terminal in the stack and there you push the right side of the rule into the stack. Okay. So if none of those, for example, you cannot match a terminal or you cannot find production rule to predict. In this case, basically the syntax is wrong. So you have to produce error message. Okay. So in this case. So here's a table actually, which tells you what to do. So so let's say this is the top of stack, you have a non terminal here and this table only refracts the prediction product because the matching product is trivial. So you have program now if this is on top of a stack and then if you see the ID here and you apply production rule, what? Okay. So basically which tells you now you have a statement list, you have to pop, you have to push down the statement list. And so if you have statement list, you see ID, not basically use rule two, which basically says it's a statement list and a statement. Okay. So also force. So if you want to check the details, you can go back a few slides and match that with the rules. Okay. So these are the rule numbers here and which tells you what to push to the stack essentially replace the top element of the stack with the right hand side expression and push those onto the stack. Okay. You keep doing that. That's for prediction. Okay. These are all telling prediction. But if you match the terminal, which are not shown here, then you just pop up the terminal. And so whatever you pop up, these need to be saved was it's terminal, non terminal and eventually these will be used to build a pass tree. Of course, you can build a pass tree at the same time. It basically it's a strategy, whatever you can do and then make any difference. So as I just mentioned, to keep track the level most non terminal, you push some scene, you have not seen the right side expression of a rule onto the stack. Okay. So you can see a little more details in the textbook in this figure. And so it is important to remember basically the stack will contain all the stuff you expect to see between now and end of program. Okay. Of course, not at once, but dynamically. Okay. So you add something and a pop up something you push down something pop up something whenever you pop up the base, I used to build this derivation tree or pass tree. Okay. So what happens if we want to do one pass in, but the given grammar is not given grammar is not right recursive, it's left recursive. So what we have to do is first we have to convert this grammar into equivalent right recursive grammar. And then we can't plot LL1 passing case in this case. So for example, in this case, if we have ID list, goes to ID list, ID list, followed by ID. Okay. So you see you can have a sequence of IDs here, but this rule itself is left recursive. Because ID list in the second rule appears on the left most side. Okay. So it's a left recursive, if it's a left recursive, it is not suitable for LL passing case in this case. So however we can basically mechanically translate that into ID list, ID list, and then ID list tells that it's the comma and then followed by ID and followed by ID list, tell, all empty. So now one rule in the left recursion is translate into, you know, essentially three rules, right? One, two, and three. The last one is a blank. Okay. And which now is right recursive. Okay. So you have ID list, tell, ID list tell, which appears on the right most side of the rule. Okay. And so by doing that, now you can apply the LL passing to solve the problem, okay, to successfully generate the pass tree. So one thing is how we can turn the left recursive grammar into right recursive graph. And another technique is to deal with common prefix, okay. So you know, basically if we have to look more than one token in this case, then we have trouble, right? So we cannot determine, okay. So we can only look at at the most one. So the reason we need to look ahead is because you have the same token, but the token behind can determine a different decisions in this case. So it's not unique. So in order to make sure our grammar can process that by looking just the one token ahead. So we can do something called a left factory. Okay. Here's example. Okay. So have statement, I, I, okay, a statement is defined by either assignment statement or a function called. Okay. So, so you see ID here, followed by assignments operation and ID here, followed by a left the crisis case. So if you just look at ID, you cannot tell what to do. Okay, was it assignment or was it's a function called? You have to look at one token ahead, right? So we can reserve this problem by left the factoring. So we have statement ID, then we have ID statement tail. So in ID statement, tell now we can have whatever remaining part of this rule we put here. And a remaining part of this rule, we put behind this. Okay. So now, so everything is unique. So we basically process that we can do a derivation and push this ID statement tail onto the stack and then whenever the prime ID statement tail meets with this assignment operation, then and no, basically it's assignment statement where it sees the left, press and it knows it's a function called. Okay. So this can be done again, mechanically. Okay. So basically can transform a grammar by left the factoring to make sure, you know, the non uniqueness thing is resolved. Okay. So I think in our whole assignment, you will have a question related to this. So make sure if you have questions, you can go back to this and just get some concept. If you still have doubt, you can read a little more to understand that. Okay. However, okay. So these are the common problems. If we, let's say we animate left recursion, common prefix and it may not always work. Okay. So that depends on whether the language itself is is non LL or not. Okay. So essentially that infinite many language which are not LL. Okay. So no matter what you do, you know, resolve left recursion, common prefix, you still can pass them using LL techniques. Okay. So but in most cases, you know, we don't have that problem. If we have, in some language, we have that problem. We can't always, you know, use some special tricks to resolve them. Okay. So here are some examples in how a language may not be LL1. And so one famous example is called a dangling else. Okay. So, so in this case, okay. It doesn't matter how many tokens you look ahead, you cannot still determine how you do it. Okay. And of course, this actually appeared in some real programming language, you know, Pascal. It's a well-known language. And the reason it's ambiguous, that means you cannot make this. Okay. So let's look at what does this mean exactly? Okay. That statement, which is if statement, okay. So we understand the key, what if, or by some kind of condition, then we have a zan class or else class. Okay. And then you can have other statement, you know, but let's focus on this if statement. Okay. So the zan class can have zan and then followed by statement. The else class now has else and statement. Okay. So that's not problem. But it can also have an option called epsilon. Okay. Absalom basically here means empty, right? So if the else part is empty, that cost problem because we can't have if then some statement. Okay. So let's say another if. Okay. So if condition, then if condition, then statement, else statement. Okay. So although that else statement should appear there with the first then the second thing, right? So that costs confusion because the grandma allows that. Okay. So there's, you know, maybe intentionally when somebody write a program, they may not aware, you know, exactly that will cause problem for the compiler to determine. So all we can do is basically we can always deal with this kind of problem by, you know, say using some special rules. Okay. Also the grandma is ambiguous. Okay. But we can always treat some special rules to deal with that. For example, we, you know, always, you know, pair the else to the closest then. Okay. So in this case, of course, the second then. So there's an ambiguity or more generally we can have two rules. Okay. So basically there are two rules, right? Then have one. The else, you know, which has the else part of empty one. Okay. So we can always use say the first one to use. So in this case, again, we look at the else. We always pair that with the, the second thing. Because we use the first rule in the else part, else class, okay, which is the else statement, not the the absolute, not the empty one. Okay. So this is how we deal with some intentional design for us in the language. But you know, in some modern languages case. So this problem can be resolved by just to introduce these explicit. Delimulators for the statements, for example, you try to always have an end to close if statement. And of course, in some languages, not necessarily using and the is, you know, if or. Okay. So the reverse the spelling of if, okay, to signify that's the end of this particular statement. If you use that, so you will never have an ambiguity occur in your program. So we understand it now. What does our pass it mean and basically it has. Push it on top of that then you have these rules, you do prediction, they used to pass. Okay. So actually how we can actually build these. You know, predict the set. Okay. So basically say enough. Build that table, right? We have that table and predict what to use. And so we can use something well known basically calculate some. Use these functions got a first. Farrow. And to predict, compute these to. For the symbols. Okay, which include nonterminal term symbols and then follow just for nonterminal symbols. So these functional wall. And which are defined by these rules essentially. And a first of this rule alpha, okay, you focus on this left hand side. Basically says if you have a sequence of derivation, it's the, you know, first. Whatever. Nonterminal terminal K a and a collected it's the first appear there. And appear in the first. Derivation. And then if you have this case. So follow up case. So in this rule, it's from the. The star symbol case, you can have a sequence of alpha case or some, some strings there. Some tokens there. And so what follows this a is again, this a here. Okay, the, the small after capital. Okay, so you can define this and then of course you can use the predictor set calculate. And a question of combination of first and follow. So these are one. You don't need to actually look at details, but if you want to look at details, these are computed actually there quite straightforward. And here are some examples from that language we just saw. And for example, I just mentioned few things. Okay, so. So let's say you have a statement statement, statement list. Okay, so in this case. What follows statement list? Okay, so the first in the statement list, basically. You only have have basically the empty right. And the symbol. Okay, so no other IDs are the terminal. Okay, but if you look at the first in the statement case, so first in the statement here. Basically in the assignment statement. You have ID case or that appear. The first in this statement, okay, and similarly in this statement, the first is the read. The first is right. Okay, so we have some idea of a first. And let's look at the way to follow. Okay, so what follows ID here for what follows ID is this assignment symbol right. And in this statement, what follows ID what follows read is ID. Okay, so these are actually fairly straightforward and easy to understand. Of course, the detailed computation is a little involved, but first these progress, you know, they are for machine to to process enough for human to read. And this is the continuation. So, you know, these actually rules are all billed by these functions. Okay, so you predict it will generate this set of rules. Find the auto what's in verse, you know, set, you know, first, follow and predict the end of song. So force. So when we do our passing, if an token belongs to the predict the set of more than one production with the same left hand side, then the grammar is not LL1K of course in this case, it's very easy to understand. So if you have some token belong to to the predict set of one more production in this case, and then you don't know which production to use right because it costs ambiguous. So the predict can also arise because same token belong begin more than one right hand side. Or as a case, you know, it can also appear after left hand side in some valid program. And for example, like absolute empty one. Okay, so these are just some general rules after you apply those functions to compute these. And you get the information and then you can do a check and I see what's the grammar actually is suitable for LL1 passing or not. Okay, so we have some idea about LL passing. Okay, so the other one we're going to briefly mention is our passing. Okay, so this is pretty much table driven. So very similar to the LL posture, but the LR posture actually is a little more complex case still is a group you know keep looking for. So this is the reduction case of application reduction rule. And in this case, it's inspect the two dimensional table to find what action to take just like the LL passively saw tables there. However, unlike the LL passer the LR driver has a non trivial state machine. Okay, so basically in the LL passer, it's very simple. In this case, because you just do prediction prediction, okay, you don't, you know need to keep memorizing things. Okay. And in the LR passing actually, so you need to have deterministic finite state, a time to keep track of where you are at this moment. Okay, so because it's in some sense, it's a bottom up so you don't have the overall picture. Okay, so you need to know the context, okay, which place currently is being reduced. Okay, so this is indexed by the current input token and current state. Okay, so it's a little more complex than the finite state machine we discussed during the scanning. Okay, so we only need to have the input character, but here we need to have token and a current state. Okay, so we need a bolt. And so state contains a record of what has been seen so far. Okay, not what is expected. Okay, so it's very different. Okay, so so here. What's in the stack is what you have already seen so far. Okay. And it's not like the stack in the L passing in the L passing where we put something we haven't seen yet. Okay, so this is is a little bit different. Okay, so make sure you understand that what what's in the stack is different because when it's top down when it's bottom up. Okay, so we save different type of information. So basically push that time up can be defined as a state diagram. Okay, so we have basically stay machine and a stack. Okay, so we, you know, push down, Thomas is basically. It's a deterministic machine plus a stack. Okay, so that's why you see the power off push down a time which is is. You know determines the finite state machine plus stack so you need to have this additional mechanism storage mechanism. Okay, so there is no difference essentially when we look at the state diagram of the atomic finite state automat. Accepted the arcs are now labeled away input symbol as well as the top of the stack symbol. Okay, so you need to not only look at input token, which you are considering right now, but you also need to look at the top of the stack. Okay, so what what what you try to do at this moment, okay, so so there are a pair of input. Okay, so in this case. And then you make a decision either you push in pop. And you know, for the symbols onto off the stack actually you can do more than what you can push in sometimes because you need to push a bunch of things into or you can't pop a bunch of things when you do the reduction. You will pop multiple symbols to to satisfy a rule when you push you could also push more than one symbol into the stack. So we just mentioned the LL also is push the atomic but in this case the state machine itself is very simple. Okay, so we really don't need to remember much because everything can can be inferred from the stack so the stack actually contains more information we need because we do the left most case we always you know do whatever we can do. Whatever we can do and do a reduction case so so you don't the context in this case is is much smaller. Okay, so you always do the current thing you can make a decision to do a reduction. Okay, so you can process that. Okay, however, when you do our so you have to basically push a lot of things into the stack then you know because the right most right right most means. You know if you don't see it yet everything need to be saved on the stack then when you reach the right most non terminal then you can do the reduction but by that time you lots of things already on stack so now you need to figure out. You know how much you need to to process you know at a time okay but when you have LL basically the left most case so you just look at the current top of stack and whatever the input that you make make a reduction in that case you make a prediction. Okay, so typically you can imagine you know the push down a time at in the LL L L passing is is shallow that means you don't need to say much onto the stack but L are passing you have to essentially save almost everything into stack at the stack will be very big in the very beginning. Because you need to see the right most of one okay and then it's not to reduction okay so you need to you know normally it has a deeper stack okay when you do our pressing kill. So a builds basically our pda. I have multiple states okay so builds the past tree from bottom up okay and need to keep track which production might be in the middle that's that's what I just mentioned in the middle okay so so you need to aware larger context in this case okay. So this this same machine we call characteristic okay final state machine is used based on shift and reduce basically you know you use that the state decide you know was it we should the shift or was we should reduce okay. And then we have the same language but defined by a slight different grammar of course you know when we talk about our question now the grammar is left the recursive you can see now the statement list is statement list statement case you can see on the right hand side of the statement list appears on the left most side okay. They have statement case of the grammar actually is need more more easy for human to understand because every sin you know like if we have expression and they have operation in the middle and then you have another operand okay so every sin is in one rule not like the right recursive one and so this rule will be broken into two rules sometimes even three ways so one is the empty. Okay so this is the rest of the grammar okay and then you can look at this basically is the the stack is not shown here but this is the part of the same machine which basically is used to to the match and which is used to examine the current based on current state and then current input to see what we are going to see. To see what to decide okay what to do okay so in this particular diagram only reduction actions are shown the shift the shift is not shown in this one okay so for example you start with a stage zero if you see our stem for read the statement right so you read the statement that's fine okay so you have read and then you go to stay one of course it's condensed here so you what you can see that's not the same thing. I expect you know basically read some information there and then if you see ID now you expect a silent statement then followed by a silent symbol and then followed by some kind of expression E stands for expression and expression can have a rhythmic addition operations there may have some kind of multiplication operation there and some supports okay so basically this give you a little peak of. You know what does the stay machine look like which is used to control the actions of the overall which don't. Okay that's all for today thank you.